{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03b1bc26",
   "metadata": {},
   "source": [
    "# Modelo bolsa de palabras para sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de043dec",
   "metadata": {},
   "source": [
    "## Planteamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c205ad",
   "metadata": {},
   "source": [
    "Una técnica popular para desarrollar modelos de análisis de sentimientos es utilizar un modelo de bolsa de palabras que transforma los documentos en vectores donde a cada palabra del documento se le asigna una puntuación. Vamos a ver cómo:\n",
    " - Preparar los datos del texto para modelar un vocabulario restringido.\n",
    " - Utilizar el modelo de bolsa de palabra para preparar datos train/test.\n",
    " - Desarrollar un modelo de bolsa de palabras de perceptrón multicapa.\n",
    " \n",
    "Entonces, vamos a dividirlo en los siguientes apartados:\n",
    " 1. Conjunto de datos de reseñas de películas\n",
    " 2. Preparación de datos\n",
    " 3. Representación de bolsa de palabras\n",
    " 4. Modelos de aprendizaje\n",
    " 5. Comparación de métodos de puntuación de palabras\n",
    " 6. Predicción en nuevas reseñas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224020b1",
   "metadata": {},
   "source": [
    "# 1. Conjunto de datos de reseñas de películas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e114b676",
   "metadata": {},
   "source": [
    "Vamos a seguir utilizando el mismo dataset que en la parte anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb24c2a9",
   "metadata": {},
   "source": [
    "# 2. Preparación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8376f168",
   "metadata": {},
   "source": [
    "Como tenemos en cuenta la limpieza anterior nos vamos a centrar en:\n",
    " - División en train/test\n",
    " - Cargar y limpiar los datos para eliminar puntuación y números\n",
    " - Definir un vocabulario de palabras preferidas\n",
    " \n",
    "Vamos a decidir si una reseña dada por un usuario es buena o mala en función del texto que ponga. Esto quiere decir que para nuevos datos no etiquetados hay que realizar el mismo preprocesamiento de datos que en el conjunto de entrenamiento.\n",
    "\n",
    "Haremos una división 90/10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9376fd3d",
   "metadata": {},
   "source": [
    "## 2.1 Carga y limpieza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc64c1de",
   "metadata": {},
   "source": [
    "Ya hemos observado que el texto está bastante limpio, por lo que no requiere mucha preparación. Entonces, vamos a realizar el siguiente preprocesamiento:\n",
    "  - Dividir tokens en espacios en blanco\n",
    "  - Eliminar toda puntuación de las palabras\n",
    "  - Eliminar todas las palabras que no estén compuestas únicamente por caracteres alfanuméricos\n",
    "  - Eliminar todas las palabras conocidas vacías\n",
    "  - Eliminar las palabras cuya longitud sea $\\leq$ 1\n",
    "  \n",
    "Para esto reutilizamos las funciones anteriores:\n",
    "  - `load_doc()`, carga un documento desde un fichero listo para utilizar con la función `clean_doc()`\n",
    "  - `clean_doc()`, recibe por parámetro un texto sin procesar y devuelve una lista de tokens limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "605aaf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffcbf441",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "def clean_doc(text):\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Eliminamos puntuación\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    re_punc = [re_punc.sub('', w) for w in tokens]\n",
    "    \n",
    "    # Eliminamos números\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Eliminamos las stop words\n",
    "    stop_words = set(stopwords.words('English'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    \n",
    "    tokens = [w for w in tokens if len(w) > 1]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfbdc74",
   "metadata": {},
   "source": [
    "## 2.2 Definir un vocabulario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8b3a65",
   "metadata": {},
   "source": [
    "Es importante limitar las palabras a aquellas que se crea que tenga poder predictivo.\n",
    "\n",
    "Para esto vamos a reutilizar las funciones vistas. Primero, desarrollamos un vocabulario a modo de `Counter` y cada fragmento se puede agregar al contador y podemos cargar todas las reviews en un fichero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8e45f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c34cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory, vocab):\n",
    "    for filename in listdir(directory):\n",
    "        if not filename.endswith('.txt'):\n",
    "            continue\n",
    "    \n",
    "        path = directory + filename\n",
    "        add_doc_to_vocab(path, \n",
    "                         vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98b26b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37589\n",
      "[('film', 8849), ('one', 5514), ('movie', 5429), ('like', 3543), ('even', 2554), ('good', 2313), ('time', 2280), ('story', 2110), ('would', 2041), ('much', 2022), ('also', 1965), ('get', 1920), ('character', 1902), ('two', 1824), ('characters', 1813), ('first', 1766), ('see', 1726), ('way', 1668), ('well', 1654), ('make', 1590), ('really', 1556), ('films', 1513), ('little', 1487), ('life', 1467), ('plot', 1448), ('people', 1418), ('could', 1395), ('bad', 1372), ('scene', 1372), ('never', 1360), ('best', 1298), ('new', 1275), ('many', 1267), ('scenes', 1262), ('man', 1255), ('know', 1207), ('movies', 1180), ('great', 1138), ('another', 1111), ('love', 1087), ('go', 1073), ('action', 1073), ('us', 1065), ('director', 1054), ('something', 1047), ('end', 1044), ('still', 1037), ('seems', 1032), ('back', 1031), ('made', 1025)]\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter()\n",
    "pos_path = 'Data/review polarity/pos/'\n",
    "neg_path = 'Data/review polarity/neg/'\n",
    "\n",
    "process_docs(pos_path, vocab)\n",
    "process_docs(neg_path, vocab)\n",
    "\n",
    "print(len(vocab))\n",
    "\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f809975",
   "metadata": {},
   "source": [
    "Ahora vamos a eliminar todas las palabras que tengan menos de 2 ocurrencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20e8e2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24392\n"
     ]
    }
   ],
   "source": [
    "umbral = 2\n",
    "tokens = [k for k,c in vocab.items() if c>=umbral]\n",
    "\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3839b2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename,\n",
    "                'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52ec3278",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = 'vocab.txt'\n",
    "\n",
    "save_list(tokens, vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e024d3",
   "metadata": {},
   "source": [
    "# 3. Representación de la bolsa de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17177b52",
   "metadata": {},
   "source": [
    "Aspectos a tener en cuenta:\n",
    "  - Cada documento se convierte en una representación vectorial.\n",
    "  - El número de elementos en el vector que representa un documento corresponde al número de palabras en el vocabulario.\n",
    "  - Cuanto mayor sea el vocabulario, mayor será la representación del vector, de ahí la preferencia por tener vocabularios más reducidos.\n",
    "\n",
    "Ahora, vamos a convertir las reviews en vectores preparados para entrenar un modelo inicial de red neuronal, es decir:\n",
    "  1. Convertir reseñas en tokens\n",
    "  2. Codificarlas con una representación de modelo de bolsa de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57c486b",
   "metadata": {},
   "source": [
    "## 3.1 Reviews a líneas de tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646e7179",
   "metadata": {},
   "source": [
    "Lo primero es realizar la limpieza de los documentos. Para ello, creamos la función `doc_to_line()` que cargará el documento, lo limpiará, filtrará los tokens que no están en el vocabulario y devolverá el documento como una cadena de tokens separados por espacios en blanco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "217c6ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_line(filename, vocab):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c600e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory, vocab):\n",
    "    lines = list()\n",
    " \n",
    "    for filename in listdir(directory):\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        \n",
    "        path = directory + filename\n",
    "        line = doc_to_line(path, vocab)\n",
    "        lines.append(line)\n",
    "        \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa03d00",
   "metadata": {},
   "source": [
    "Ahora vamos a crear una función `clean_load_dataset()` que llamará a `process_docs()` para construir el dataset en el que las revisiones positivas son 1 y las negativas 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62568738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_dataset(vocab):\n",
    "    neg = process_docs('Data/review polarity/neg/', \n",
    "                       vocab)\n",
    "    pos = process_docs('Data/review polarity/pos/', \n",
    "                       vocab)\n",
    "    \n",
    "    docs = neg + pos \n",
    "    \n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceb3cac",
   "metadata": {},
   "source": [
    "Finalmente, tenemos que cargar el vocabulario y convertirlo en un conjunto para utilizar en revisiones de limpieza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff87b506",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = load_doc(vocab_path)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ba9baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, labels = load_clean_dataset(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d570b4e2",
   "metadata": {},
   "source": [
    "## 3.2 Reviews de películas a vectores de bolsa de palabra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6597e95e",
   "metadata": {},
   "source": [
    "La clase `Tokenizer` es conveniente y transformará fácilmente los documentos en vectores codificados. Primero, se debe crear el `Tokenizer` y, luego, realizar el `fit`.\n",
    "\n",
    "En este caso, se trata de la agregación de matrices positive_lines y negative_lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "864a02fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742c0885",
   "metadata": {},
   "source": [
    "Los documentos se pueden codificar mediante `Tokenizer` con la función `texts_to_matrix()`, que tiene 2 parámetros:\n",
    "  - Lista de documentos para codificar\n",
    "  - Modo de codificación\n",
    "      · Aquí específicamos `freq` para puntuar palabras en función de su frecuencia en el momento\n",
    "      \n",
    "Vamos a tener que modificar la función `process_docs()` para procesar de forma selectiva las reviews en train_test.\n",
    "  - Admitimos la carga de train/test agregando un argumento `is_train` y usándolo para decidir qué nombres de archivos omitir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "031c81d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory, vocab, is_train):\n",
    "    lines = list()\n",
    " \n",
    "    for filename in listdir(directory):\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        \n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        \n",
    "        path = directory + filename\n",
    "        line = doc_to_line(path, vocab)\n",
    "        lines.append(line)\n",
    "        \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67736d48",
   "metadata": {},
   "source": [
    "Igualmente, vamos a tener que modificar la función `load_clean_dataset()` para cargar el train o test y asegurar que devuelva una matriz NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ffe3516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_dataset(vocab, is_train):\n",
    "    neg = process_docs('Data/review polarity/neg/', \n",
    "                       vocab,\n",
    "                       is_train)\n",
    "    \n",
    "    pos = process_docs('Data/review polarity/pos/', \n",
    "                       vocab,\n",
    "                       is_train)\n",
    "    \n",
    "    docs = neg + pos \n",
    "    \n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e46a65",
   "metadata": {},
   "source": [
    "Ahora vamos a cargar todas las reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a15c59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 24208) (200, 24208)\n"
     ]
    }
   ],
   "source": [
    "train_docs, y_train = load_clean_dataset(vocab, \n",
    "                                         True)\n",
    "test_docs, y_test = load_clean_dataset(vocab, \n",
    "                                       False)\n",
    "\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "\n",
    "x_train = tokenizer.texts_to_matrix(train_docs,\n",
    "                                   mode='freq')\n",
    "\n",
    "x_test = tokenizer.texts_to_matrix(test_docs,\n",
    "                                  mode='freq')\n",
    "\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4b47b5",
   "metadata": {},
   "source": [
    "Vemos que train y test tienen 1800 y 200 documentos, respectivamente, y cada uno con igual tamaño de vocabulario de codificación, es decir, 24208."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ff4970",
   "metadata": {},
   "source": [
    "# 4. Modelos de análsis de reseñas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de35132f",
   "metadata": {},
   "source": [
    "Vamos a implementar un modelo perceptrón multicapa para clasificar documentos codificados como positivos o negativos. Vamos a tener 3 partes:\n",
    "  1. Primer modelo de análisis de reseña\n",
    "  2. Comparación de los modos de puntuación de palabras\n",
    "  3. Predicción para nuevas reseñas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7d3d86",
   "metadata": {},
   "source": [
    "## 4.1 Modelo de análisis de primera opinión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a2b73c",
   "metadata": {},
   "source": [
    "El modelo tendrá:\n",
    "  - Una capa de entrada igual al número de palabras del vocabulario y, a su vez, la longitud de los documentos de entrada\n",
    "  - Una sola capa oculta de 50 neuronas con función de activación ReLu\n",
    "  - La capa de salida consta de una única neurona con función de activación sigmoidea\n",
    "  - Optimizador ADAM\n",
    "  - Función de pérdida `binary_crossentropy`\n",
    "  - Evaluaremos accuracy al entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31e5c3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "\n",
    "def base_model(n_words):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50,\n",
    "                   input_shape=(n_words, ),\n",
    "                   activation='relu'\n",
    "                   ))\n",
    "    model.add(Dense(1,\n",
    "                    activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    plot_model(model,\n",
    "               to_file='model.png',\n",
    "               show_shapes=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27b4fcfd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 50)                1210450   \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1210501 (4.62 MB)\n",
      "Trainable params: 1210501 (4.62 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "Epoch 1/10\n",
      "57/57 - 1s - loss: 0.6919 - accuracy: 0.5461 - 701ms/epoch - 12ms/step\n",
      "Epoch 2/10\n",
      "57/57 - 0s - loss: 0.6834 - accuracy: 0.6550 - 377ms/epoch - 7ms/step\n",
      "Epoch 3/10\n",
      "57/57 - 0s - loss: 0.6698 - accuracy: 0.6156 - 362ms/epoch - 6ms/step\n",
      "Epoch 4/10\n",
      "57/57 - 0s - loss: 0.6471 - accuracy: 0.8522 - 359ms/epoch - 6ms/step\n",
      "Epoch 5/10\n",
      "57/57 - 0s - loss: 0.6173 - accuracy: 0.9006 - 366ms/epoch - 6ms/step\n",
      "Epoch 6/10\n",
      "57/57 - 0s - loss: 0.5805 - accuracy: 0.9244 - 367ms/epoch - 6ms/step\n",
      "Epoch 7/10\n",
      "57/57 - 0s - loss: 0.5391 - accuracy: 0.9550 - 384ms/epoch - 7ms/step\n",
      "Epoch 8/10\n",
      "57/57 - 0s - loss: 0.4958 - accuracy: 0.9583 - 383ms/epoch - 7ms/step\n",
      "Epoch 9/10\n",
      "57/57 - 0s - loss: 0.4529 - accuracy: 0.9578 - 392ms/epoch - 7ms/step\n",
      "Epoch 10/10\n",
      "57/57 - 0s - loss: 0.4121 - accuracy: 0.9650 - 365ms/epoch - 6ms/step\n",
      "Accuracy en test: 88.000000 %\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "\n",
    "n_words = x_test.shape[1]\n",
    "y_train = array(y_train)\n",
    "y_test = array(y_test)\n",
    "\n",
    "model = base_model(n_words)\n",
    "\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          epochs=10,\n",
    "          verbose=2)\n",
    "\n",
    "loss, acc = model.evaluate(x_test,\n",
    "                           y_test,\n",
    "                           verbose=0)\n",
    "\n",
    "print('Accuracy en test: %f' % (acc*100), '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
