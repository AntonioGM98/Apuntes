{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af1270ae",
   "metadata": {},
   "source": [
    "# Contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03abab71",
   "metadata": {},
   "source": [
    "El texto debe analizarse para eliminar palabras (**tokenización**). Luego, las palabras deben codificarse como números enteros o valores de punto flotante para usar como entrada en un algoritmo de ML, i.e., extracción de características (**vectorización**).\n",
    "Aquí nos centramos en :\n",
    "\n",
    "**·** Convertir texto en vectores de conteo de palabras con `CountVectorizer`\n",
    "\n",
    "**·** Convertir texto a vectores de frecuencia de palabras con `TfidVectorizer`\n",
    "\n",
    "**·** Convertir textos enteros únicos con `HashingVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4ad1a2",
   "metadata": {},
   "source": [
    "# Modelo de la bolsa de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27af2fc5",
   "metadata": {},
   "source": [
    "No se puede tratar directamente con texto cuando utilizamos algoritmos de ML, sino que debemos convertir el texto a número. \n",
    "Los algoritmos toman vectores de números como entrada, por lo tanto, necesitamos convertir el documento en vectores de \n",
    "números de longitud fija.\n",
    "\n",
    "El modelo de bolsa de palabras elimina toda la información de orden en las palabras y se enfoca en la aparición de palabras\n",
    "en un documento, asignando a cada palabra un número único. Entonces:\n",
    "\n",
    "    - Cualquier documento que puede codificarse como un vector de longitud fija con la longitud de vocabulario de palabras \n",
    "    conocidas.\n",
    "    - El valor en cada posición en el vector podría completarse con un recuento o frecuencia de cada palabra en el\n",
    "    documento codificado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7154d4e3",
   "metadata": {},
   "source": [
    "# Contar palabras con `CountVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fd09a8",
   "metadata": {},
   "source": [
    "`CountVectorizer` proporciona:\n",
    "\n",
    "    - Una forma sencilla de tokenizar una colección de documentos de texto y crear un vocabulario de palabras conocidas.\n",
    "    - Codificar nuevos documentos utilizando ese vocabulario.\n",
    "    \n",
    "Uso:\n",
    "\n",
    " 1. Crear una instancia de la clase `CountVectorizer`\n",
    " 2. Llamar a la función `fit()` para aprender un vocabulario de uno o más documentos\n",
    " 3. Llamar a la función `transform()` en uno o más documentos según sea necesario para codificar cada uno como un vector\n",
    " 4. Se devuelve un vector codificado con la longitud de todo el vocabulario y un número entero para el nº de veces que ha           aparecido cada palabra en el documento\n",
    "      · Como contienen muchos 0's, los denominamos vectores dispersos\n",
    "      · El paquete `scipy.parse` proporciona formas eficientes de trabajar con estos vectores dispersos\n",
    " 5. Los vectores devueltos por una llamada a `transform()` serán vectores dispersos.\n",
    " 6. Se pueden volver a transformar en matrices NumPy con la función `toarray()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71612bb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'texto': 8, 'de': 3, 'prueba': 7, 'para': 4, 'primera': 5, 'toma': 9, 'contacto': 1, 'con': 0, 'countvectorizer': 2, 'probar': 6}\n",
      "\n",
      "Shape:  (1, 10) Type: <class 'scipy.sparse._csr.csr_matrix'> Array:  [[1 1 1 2 2 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = ['Texto de prueba para primera toma de contacto con CountVectorizer para probar']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorizer.fit(text)\n",
    "\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "print()\n",
    "print('Shape: ', vector.shape, 'Type:', type(vector), 'Array: ', vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91453fb",
   "metadata": {},
   "source": [
    "Detalles:\n",
    "- Todas las palabras se escriben en minúsculas\n",
    "- Ignora la puntuación\n",
    "- Tenemos 10 palabras en el vocabulario\n",
    "- Tenemos un vector de longitud del vocabulario\n",
    "- Se codifican las palabras por orden alfabético\n",
    "\n",
    "El mismo `vectorizer` se puede utilizar en otros documentos:\n",
    "- Si no se incluyen en su vocabulario, se ignoran\n",
    "- Si están incluidas, las tiene en cuenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b5b50f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "text2 = ['Texto prueba fuera vector']\n",
    "vector = vectorizer.transform(text2)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d6b7c0",
   "metadata": {},
   "source": [
    "Como podemos ver los 1's corresponden con las posiciones 7 y 8, es decir, prueba y texto, respectivamente, que sabíamos que pertenecían al vocabulario original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400711a7",
   "metadata": {},
   "source": [
    "# Frecuencia de palabras con `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4868aee",
   "metadata": {},
   "source": [
    "Un problema con los recuentos simples es que palabras como *the* aparecerán muchas veces cuando no añade información de contexto.\n",
    "\n",
    "Para mitigarlo, se utiliza el método Term Frequency - Inverse Document Frequency (TF-IDF), que significa:\n",
    "- **Frecuencia de términos**: ¿Con qué frecuencia el término aparece en este documento? Mientras mayor sea la frecuencia del término en el documento, mayor será su importancia.\n",
    "- **Frecuencia de documentos inversa**: ¿Con qué frecuencia el término aparece en todos los documentos de la colección? Mientras mayor sea la frecuencia en los documentos, menor será su importancia.\n",
    "\n",
    "`TfidfVectorizer` tokenizará los documentos, aprenderá el vocabulario y el documento inverso sobre ponderaciones de frecuencia y permitirá codificar nuevos documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "921c0879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: {'texto': 10, 'de': 4, 'prueba': 8, 'para': 5, 'primera': 6, 'toma': 12, 'contacto': 2, 'con': 1, 'countvectorizer': 3, 'probar': 7, 'computer': 0, 'science': 9, 'tfidfvectorizer': 11}\n",
      "idf: [1.69314718 1.         1.69314718 1.28768207 1.69314718 1.69314718\n",
      " 1.69314718 1.69314718 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718]\n",
      "Shape: (1, 13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.15507331, 0.26256193, 0.19968512, 0.52512386,\n",
       "        0.52512386, 0.26256193, 0.26256193, 0.19968512, 0.        ,\n",
       "        0.26256193, 0.        , 0.26256193]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "'''\n",
    " En cada texto, vamos a poner una palabra que exista en el primero junto con un conector y una, o más, palabras\n",
    " que no existan en el primero\n",
    "'''\n",
    "\n",
    "text = ['Texto de prueba para primera toma de contacto con CountVectorizer para probar', \n",
    "        'Prueba con computer science',\n",
    "        'CountVectorizer con TfidfVectorizer']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(text)\n",
    "\n",
    "print('Vocabulario:',vectorizer.vocabulary_)\n",
    "print('idf:',vectorizer.idf_)\n",
    "\n",
    "vector = vectorizer.transform([text[0]])\n",
    "print('Shape:', vector.shape)\n",
    "vector.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca46708",
   "metadata": {},
   "source": [
    "[Mas informacion de TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3473c",
   "metadata": {},
   "source": [
    "Detalles:\n",
    "- Aprende un vocabulario de 13 palabras\n",
    "- A cada palabra le asigna un índice entero único en el vector de salida\n",
    "- Las fecuencias inversas del documento se calculan para cada palabra del vocabulario\n",
    "- Se le asigna la puntuación más baja de 1 a la palabra con mayor frecuencia: *con* en el índice 1\n",
    "- Las palabras que aparecen en el resto de texto tienen menor valor(*CountVectorizer* y *Prueba*)\n",
    "- El primer documento se codifica como una matriz dispersa de 13 elementos con el puntaje de cada palabra\n",
    "- Las palabras que aparecen con 0 son las que no se encuentran\n",
    "- Las puntuaciones se normalizan con valores entre 0 y 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1232dd",
   "metadata": {},
   "source": [
    "# Hashing con `HashingVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11df69a3",
   "metadata": {},
   "source": [
    "Los conteos y las frecuencias tienen la limitación de que el vocabulario se vuelve muy extenso y requerirá grandes vectores para codificar documentos que requerirán, a su vez, grandes requisitos de memoria.\n",
    "\n",
    "Una solución es utilizar un hash de palabras unidireccional para convertirlas en números enteros:\n",
    "- No requiere vocabulario\n",
    "- Se puede elegir un vector de longitud fija arbitraria \n",
    "- Inconveniente: Es una función unidireccional, no podemos volver atrás\n",
    "\n",
    "La clase `HashingVectorizer` implementa este enfoque que se puede usar para hash palabras, luego tokenizar y codificar documentos según sea necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fae43392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "[[ 0.    0.25  0.    0.    0.    0.25  0.   -0.25  0.    0.    0.5   0.\n",
      "   0.    0.    0.    0.5   0.    0.    0.5   0.25]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "text = ['Texto de prueba para primera toma de contacto con CountVectorizer para probar']\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=20)\n",
    "\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf47be",
   "metadata": {},
   "source": [
    "La ejecución del ejemplo codifica el documento de muestra como una matriz dispersa de 20 elementos. Los valores del documento codificado corresponden a recuentos de palabras normalizados por defecto en el rango de -1 a 1, pero se pueden hacer recuentos de enteros simple cambiando la configuración predeterminada."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
