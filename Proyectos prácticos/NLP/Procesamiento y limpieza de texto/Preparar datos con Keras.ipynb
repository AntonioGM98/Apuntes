{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38f7d6c5",
   "metadata": {},
   "source": [
    "# Preparar datos con Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b5d946",
   "metadata": {},
   "source": [
    "Keras proporciona herramientas para convertir datos de formato texto a numérico para preparar un corpus que pueda ser ejecutado por los modelos. En este notebook vamos a tratar:\n",
    " - Métodos para procesar datos\n",
    " - API `Tokenizer` que codifica documentos y realiza el proceso de validación y prueba\n",
    " - Los 4 esquemas de codificación de documentos diferentes que ofrece `Tokenizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf16de9",
   "metadata": {},
   "source": [
    "## Dividir palabras con `text_to_word_sequence`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d8a15f",
   "metadata": {},
   "source": [
    "Un primer pasio trabajando con texto es dividirlo en palabras. Las palabras se llaman **tokens** y el proceso de dividir texto en tokens, **tokenización**. Keras proporciona la función `text_to_word_sequence()` que divide el texto en una lista de palabras, realizando las siguientes tareas:\n",
    "- Dividir palabras por espacios en blanco\n",
    "- Filtrar la puntuación\n",
    "- Convertir texto a minúsculas(`lower=True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98d059ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['texto',\n",
       " 'de',\n",
       " 'prueba',\n",
       " 'para',\n",
       " 'primera',\n",
       " 'toma',\n",
       " 'de',\n",
       " 'contacto',\n",
       " 'con',\n",
       " 'countvectorizer',\n",
       " 'para',\n",
       " 'probar']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "text = 'Texto de prueba para primera toma de contacto con CountVectorizer para probar'\n",
    "\n",
    "result = text_to_word_sequence(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692843ef",
   "metadata": {},
   "source": [
    "## Codificación con `one_hot`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c4961",
   "metadata": {},
   "source": [
    "Keras proporciona la función `one_hot()` que se puede utilizar para tokenizar y codificar un documento de texto en un solo paso.\n",
    "\n",
    "- El nombre sugiere que creará un One Hot Encoding, que no es el caso.\n",
    "- La función es un wrapper para la función `hashing_trick()`.\n",
    "- La función devuelve una versión codificada en enteros del documento.\n",
    "- El uso de una función hash significa que puede haber colisiones y no a todas las palabras se les asignarán valores enteros únicos.\n",
    "- `one_hot()` hará que el texto esté en minúsculas, filtrará la puntuación y dividirá las palabras en función de los espacios en blanco.\n",
    "- Además del texto, se debe especificar el tamaño del vocabulario (palabras totales).\n",
    "- El tamaño del vocabulario define el espacio hash desde el cual se codifican las palabras.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Más información sobre [one_hot()](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d27aa2e",
   "metadata": {},
   "source": [
    "Primero verificamos el tamaño del vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ee7d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probar', 'de', 'para', 'toma', 'countvectorizer', 'con', 'contacto', 'prueba', 'texto', 'primera'}\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "text = 'Texto de prueba para primera toma de contacto con CountVectorizer para probar'\n",
    "\n",
    "words = set(text_to_word_sequence(text))\n",
    "print(words)\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dfd074",
   "metadata": {},
   "source": [
    "Podemos juntar esto con la función `one_hot()` y codificar las palabras en el documento.\n",
    "- El tamaño del vocabulario se incrementa en un tercio para minimizar las colisiones al mezclar palabras\n",
    "- Se imprimer el tamaño del vocabulario como 10.\n",
    "- El documento codificado se imprime como una matriz de palabras codificadas con números enteros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc44407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 10, 3, 2, 6, 9, 10, 10, 10, 9, 2, 7]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "result = one_hot(text,\n",
    "                 n=round(vocab_size*1.3))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8ef796",
   "metadata": {},
   "source": [
    "Por la naturaleza estocástica de las redes neuronales, los resultados específicos pueden variar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc55fd6",
   "metadata": {},
   "source": [
    "## Codificación hash con `hashing_trick`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16750ff6",
   "metadata": {},
   "source": [
    "- La función `hashing_trick()` que tokeniza y luego codifica el documento con enteros, al igual que la función `one_hot()`\n",
    "- Proporciona más flexibilidad, lo que permite especificar la función hash como `hash` u otras funciones como la `md5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8acdc0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[1, 2, 7, 10, 10, 3, 2, 6, 2, 2, 10, 9]\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import hashing_trick\n",
    "\n",
    "text = 'Texto de prueba para primera toma de contacto con CountVectorizer para probar'\n",
    "\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "\n",
    "results = hashing_trick(text,\n",
    "                        n=round(vocab_size*1.3),\n",
    "                        hash_function='md5')\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb13d05",
   "metadata": {},
   "source": [
    "## API de Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61029710",
   "metadata": {},
   "source": [
    "Keras proporciona la API `Tokenizer` para preparar texto que se puede ajustar y reutilizar para preparar varios documentos de texto.\n",
    "\n",
    "`Tokenizer` debe construirse y luego caber en documentos de texto sin formato o documentos de texto codificados con enteros.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Más información sobre [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ff73944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.preprocessing.text.Tokenizer at 0x1dd851e0e80>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "docs = ['Artificial artificial intelligence',\n",
    "        'Applied Intelligence',\n",
    "        'Information fusion',\n",
    "        'Artificial Information',\n",
    "        'Computer Science']\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(docs)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f3da5",
   "metadata": {},
   "source": [
    "`Tokenizer` proporciona 4 atributos:\n",
    "- **word_counts**: diccionario de palabra y sus recuentos de ocurrencia.\n",
    "- **document_count**: número de documentos evaluados.\n",
    "- **word_index**: diccionario de palabras y sus índices únicos.\n",
    "- **word_docs**: diccionario de palabras y el número de documentos en el que aparecen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9e73375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('artificial', 2), ('intelligence', 2), ('applied', 1), ('information', 2), ('fusion', 1), ('computer', 1), ('science', 1)])\n",
      "\n",
      "5\n",
      "\n",
      "{'artificial': 1, 'intelligence': 2, 'information': 3, 'applied': 4, 'fusion': 5, 'computer': 6, 'science': 7}\n",
      "\n",
      "defaultdict(<class 'int'>, {'artificial': 2, 'intelligence': 2, 'applied': 1, 'information': 2, 'fusion': 1, 'computer': 1, 'science': 1})\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_counts)\n",
    "print()\n",
    "print(tokenizer.document_count)\n",
    "print()\n",
    "print(tokenizer.word_index)\n",
    "print()\n",
    "print(tokenizer.word_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd293eb",
   "metadata": {},
   "source": [
    "La función `texts_to_matrix()` de `Tokenizer`:\n",
    "- Crea un vector por documento provisto por entrada\n",
    "- La longitud de los vectores es el tamaño total del vocabulario\n",
    "- Proporciona el argumento `mode` que incluye:\n",
    "  - **`binary`**: Si cada palabra está presente o no en el documento. Opción por defecto.\n",
    "  - **`count`**: El conteo de palabras en el documento.\n",
    "  - **`tfidf`**: Puntuación TF-IDF para cada palabra en el documento.\n",
    "  - **`freq`**: Frecuencia de cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0fd7c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 2., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer.texts_to_matrix(docs,\n",
    "                                    mode='count')\n",
    "encoded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
